{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeep_8~9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4s4LbJWOwp7"
      },
      "source": [
        "**Perceptron**\n",
        "인공 신경망의 한 종류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIrBcT4POkSE"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda'\n",
        "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW2BWZbxF8ao"
      },
      "source": [
        "linear = torch.nn.Linear(2, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear, sigmoid).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXlfhOZ5GpwB"
      },
      "source": [
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I54woefsGwsK",
        "outputId": "0b4aee22-2954-4963-f299-68a4fbed7d2c"
      },
      "source": [
        "for step in range(10001):\n",
        "  hypothesis = model(X)\n",
        "  cost = criterion(hypothesis, Y)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(step % 100 == 0):\n",
        "    print(step, cost.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.699385941028595\n",
            "100 0.6931474208831787\n",
            "200 0.6931471824645996\n",
            "300 0.6931471228599548\n",
            "400 0.6931471824645996\n",
            "500 0.6931471824645996\n",
            "600 0.6931471824645996\n",
            "700 0.6931471824645996\n",
            "800 0.6931471824645996\n",
            "900 0.6931471824645996\n",
            "1000 0.6931471824645996\n",
            "1100 0.6931471824645996\n",
            "1200 0.6931471824645996\n",
            "1300 0.6931471824645996\n",
            "1400 0.6931471824645996\n",
            "1500 0.6931471824645996\n",
            "1600 0.6931471824645996\n",
            "1700 0.6931471824645996\n",
            "1800 0.6931471824645996\n",
            "1900 0.6931471824645996\n",
            "2000 0.6931471824645996\n",
            "2100 0.6931471824645996\n",
            "2200 0.6931471824645996\n",
            "2300 0.6931471824645996\n",
            "2400 0.6931471824645996\n",
            "2500 0.6931471824645996\n",
            "2600 0.6931471824645996\n",
            "2700 0.6931471824645996\n",
            "2800 0.6931471824645996\n",
            "2900 0.6931471824645996\n",
            "3000 0.6931471824645996\n",
            "3100 0.6931471824645996\n",
            "3200 0.6931471824645996\n",
            "3300 0.6931471824645996\n",
            "3400 0.6931471824645996\n",
            "3500 0.6931471824645996\n",
            "3600 0.6931471824645996\n",
            "3700 0.6931471824645996\n",
            "3800 0.6931471824645996\n",
            "3900 0.6931471824645996\n",
            "4000 0.6931471824645996\n",
            "4100 0.6931471824645996\n",
            "4200 0.6931471824645996\n",
            "4300 0.6931471824645996\n",
            "4400 0.6931471824645996\n",
            "4500 0.6931471824645996\n",
            "4600 0.6931471824645996\n",
            "4700 0.6931471824645996\n",
            "4800 0.6931471824645996\n",
            "4900 0.6931471824645996\n",
            "5000 0.6931471824645996\n",
            "5100 0.6931471824645996\n",
            "5200 0.6931471824645996\n",
            "5300 0.6931471824645996\n",
            "5400 0.6931471824645996\n",
            "5500 0.6931471824645996\n",
            "5600 0.6931471824645996\n",
            "5700 0.6931471824645996\n",
            "5800 0.6931471824645996\n",
            "5900 0.6931471824645996\n",
            "6000 0.6931471824645996\n",
            "6100 0.6931471824645996\n",
            "6200 0.6931471824645996\n",
            "6300 0.6931471824645996\n",
            "6400 0.6931471824645996\n",
            "6500 0.6931471824645996\n",
            "6600 0.6931471824645996\n",
            "6700 0.6931471824645996\n",
            "6800 0.6931471824645996\n",
            "6900 0.6931471824645996\n",
            "7000 0.6931471824645996\n",
            "7100 0.6931471824645996\n",
            "7200 0.6931471824645996\n",
            "7300 0.6931471824645996\n",
            "7400 0.6931471824645996\n",
            "7500 0.6931471824645996\n",
            "7600 0.6931471824645996\n",
            "7700 0.6931471824645996\n",
            "7800 0.6931471824645996\n",
            "7900 0.6931471824645996\n",
            "8000 0.6931471824645996\n",
            "8100 0.6931471824645996\n",
            "8200 0.6931471824645996\n",
            "8300 0.6931471824645996\n",
            "8400 0.6931471824645996\n",
            "8500 0.6931471824645996\n",
            "8600 0.6931471824645996\n",
            "8700 0.6931471824645996\n",
            "8800 0.6931471824645996\n",
            "8900 0.6931471824645996\n",
            "9000 0.6931471824645996\n",
            "9100 0.6931471824645996\n",
            "9200 0.6931471824645996\n",
            "9300 0.6931471824645996\n",
            "9400 0.6931471824645996\n",
            "9500 0.6931471824645996\n",
            "9600 0.6931471824645996\n",
            "9700 0.6931471824645996\n",
            "9800 0.6931471824645996\n",
            "9900 0.6931471824645996\n",
            "10000 0.6931471824645996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKcSCz16HWdG"
      },
      "source": [
        "**Multi Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umlHoBhkHEEL"
      },
      "source": [
        "X = torch.FloatTensor([[0,0], [0,1], [1,0], [1,1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "w1 = torch.Tensor(2,2).to(device)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2,1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)\n",
        "\n",
        "learning_rate = 1e-1\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0/ (1.0 + torch.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR3g2hZZSNHt"
      },
      "source": [
        "for step in range(10001):\n",
        "\n",
        "  #forward\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  a1 = sigmoid(l1)\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  cost = -torch.mean(Y * torch.log(Y_pred) + (1 - Y) * torch.log(1 - Y_pred))\n",
        "\n",
        "\n",
        "  #back prop\n",
        "  d_Y_pred = (Y_pred - Y) / (Y_pred * (1-Y_pred) + 1e-7)\n",
        "\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
        "  d_b2 = d_l2\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
        "\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "\n",
        "  w1 = w1 - learning_rate * d_w1\n",
        "  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate * d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if(step % 100 == 0):\n",
        "    print(step, cost) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiAUHrcvWmh_",
        "outputId": "8ba3a4f0-2cdb-4efb-d8a4-5002bb9e189e"
      },
      "source": [
        "#torch 함수를 이용\n",
        "\n",
        "X = torch.FloatTensor([[0,0], [0,1], [1,0], [1,1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "linear1 = torch.nn.Linear(2,10, bias = True)\n",
        "linear2 = torch.nn.Linear(10,10, bias = True)\n",
        "linear3 = torch.nn.Linear(10,10, bias = True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
        "\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
        "\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  hypothesis = model(X)\n",
        "  cost = criterion(hypothesis, Y)\n",
        "\n",
        "\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (step%100 == 0):\n",
        "    print(step, cost.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.6966626644134521\n",
            "100 0.6931644082069397\n",
            "200 0.6931622624397278\n",
            "300 0.6931602954864502\n",
            "400 0.6931583881378174\n",
            "500 0.6931565999984741\n",
            "600 0.6931548118591309\n",
            "700 0.6931530237197876\n",
            "800 0.6931513547897339\n",
            "900 0.6931496858596802\n",
            "1000 0.6931480169296265\n",
            "1100 0.6931464672088623\n",
            "1200 0.6931448578834534\n",
            "1300 0.6931432485580444\n",
            "1400 0.6931414604187012\n",
            "1500 0.6931397914886475\n",
            "1600 0.693138062953949\n",
            "1700 0.6931363344192505\n",
            "1800 0.6931345462799072\n",
            "1900 0.6931325197219849\n",
            "2000 0.6931305527687073\n",
            "2100 0.6931284666061401\n",
            "2200 0.6931262016296387\n",
            "2300 0.6931238174438477\n",
            "2400 0.6931213140487671\n",
            "2500 0.6931185722351074\n",
            "2600 0.6931155920028687\n",
            "2700 0.6931123733520508\n",
            "2800 0.6931087970733643\n",
            "2900 0.6931048631668091\n",
            "3000 0.6931005716323853\n",
            "3100 0.6930958032608032\n",
            "3200 0.6930903196334839\n",
            "3300 0.6930842399597168\n",
            "3400 0.6930773258209229\n",
            "3500 0.693069338798523\n",
            "3600 0.6930601596832275\n",
            "3700 0.693049430847168\n",
            "3800 0.6930368542671204\n",
            "3900 0.6930218935012817\n",
            "4000 0.6930038332939148\n",
            "4100 0.6929817199707031\n",
            "4200 0.692954421043396\n",
            "4300 0.692919909954071\n",
            "4400 0.6928754448890686\n",
            "4500 0.6928166151046753\n",
            "4600 0.6927363872528076\n",
            "4700 0.6926226615905762\n",
            "4800 0.692453920841217\n",
            "4900 0.6921870112419128\n",
            "5000 0.6917275190353394\n",
            "5100 0.6908309459686279\n",
            "5200 0.6886969804763794\n",
            "5300 0.6813408136367798\n",
            "5400 0.6247527599334717\n",
            "5500 0.3666766285896301\n",
            "5600 0.015109438449144363\n",
            "5700 0.0061074914410710335\n",
            "5800 0.0036467495374381542\n",
            "5900 0.002549339085817337\n",
            "6000 0.0019397512078285217\n",
            "6100 0.0015558548038825393\n",
            "6200 0.0012934852857142687\n",
            "6300 0.001103685819543898\n",
            "6400 0.0009604230872355402\n",
            "6500 0.0008486994192935526\n",
            "6600 0.0007593156769871712\n",
            "6700 0.0006862105801701546\n",
            "6800 0.0006254434119910002\n",
            "6900 0.0005741934292018414\n",
            "7000 0.0005304014775902033\n",
            "7100 0.0004925306420773268\n",
            "7200 0.0004595068749040365\n",
            "7300 0.00043046483187936246\n",
            "7400 0.0004047780530527234\n",
            "7500 0.000381834979634732\n",
            "7600 0.00036127769271843135\n",
            "7700 0.00034271838376298547\n",
            "7800 0.00032593333162367344\n",
            "7900 0.0003106392687186599\n",
            "8000 0.00029668700881302357\n",
            "8100 0.0002838678192347288\n",
            "8200 0.00027209220570512116\n",
            "8300 0.0002611961681395769\n",
            "8400 0.0002511200727894902\n",
            "8500 0.0002417446521576494\n",
            "8600 0.0002330549614271149\n",
            "8700 0.00022494661970995367\n",
            "8800 0.0002173302200390026\n",
            "8900 0.00021022066357545555\n",
            "9000 0.0002035434008575976\n",
            "9100 0.0001972834870684892\n",
            "9200 0.00019132171291857958\n",
            "9300 0.00018577731680124998\n",
            "9400 0.00018047139747068286\n",
            "9500 0.0001754934201017022\n",
            "9600 0.0001707390183582902\n",
            "9700 0.0001662529248278588\n",
            "9800 0.00016199040692299604\n",
            "9900 0.0001579067320562899\n",
            "10000 0.00015401685959659517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9gQdWY5bWzd"
      },
      "source": [
        "**ReLU, Weight Initialization, Dropout, Batch Normalization**\n",
        "\n",
        "- with MNIST_nn(xavier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LH8alD3BMMm"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljyDJl8zDpoG"
      },
      "source": [
        "torch.manual_seed(777)\n",
        "\n",
        "learning_rate = 0.01\n",
        "training_epochs = 10\n",
        "batch_size = 32\n",
        "drop_prob = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzaigdrYD2U3"
      },
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIPJdpTBFCBT"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset = mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWP69A0AFTjZ"
      },
      "source": [
        "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "bn1 = torch.nn.BatchNorm1d(32)\n",
        "bn2 = torch.nn.BatchNorm1d(32)\n",
        "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "nn_linear3 = torch.nn.Linear(32, 10, bias=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyImIb_9ZkP5",
        "outputId": "ff3746ef-f6eb-4e8c-bc50-3781b6331af2"
      },
      "source": [
        "#batch_normal + initializer + dropout\n",
        "#dropout = torch.nn.Dropout(p = drop_prob)\n",
        "\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "torch.nn.init.xavier_uniform_(nn_linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(nn_linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(nn_linear3.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.2844, -0.1696, -0.3504, -0.2301,  0.3687,  0.2130,  0.3712,  0.0127,\n",
              "         -0.0667, -0.2936,  0.1360,  0.2589,  0.0215, -0.1381,  0.3471,  0.1991,\n",
              "          0.1466, -0.1051,  0.3149, -0.3401,  0.1925, -0.3464, -0.1592, -0.3063,\n",
              "         -0.1072, -0.3241,  0.2616, -0.3744, -0.0819,  0.3308, -0.3212,  0.2470],\n",
              "        [-0.2075,  0.3301,  0.0059, -0.0219, -0.1467, -0.2131,  0.1993,  0.0217,\n",
              "          0.3126,  0.2147,  0.2938, -0.2682, -0.2865,  0.3194,  0.2741, -0.0105,\n",
              "          0.0890,  0.1457,  0.0950, -0.0853, -0.1757,  0.1373, -0.0809,  0.2979,\n",
              "          0.0628,  0.1068,  0.2325, -0.2716, -0.3309, -0.1717, -0.1876,  0.1287],\n",
              "        [ 0.0018,  0.0960,  0.1969, -0.3077,  0.3754, -0.3342,  0.2151, -0.2268,\n",
              "         -0.2043,  0.0673,  0.1056,  0.1565, -0.1447,  0.2203,  0.3615, -0.1874,\n",
              "          0.1048,  0.2576,  0.3728,  0.3112,  0.1111, -0.3100, -0.3665, -0.2684,\n",
              "         -0.2892,  0.2393,  0.0008, -0.3286,  0.0745,  0.3198, -0.1008,  0.1964],\n",
              "        [ 0.0587,  0.2191, -0.0587,  0.1542, -0.2727,  0.3528, -0.2835,  0.0203,\n",
              "          0.1000, -0.2037,  0.0868, -0.2830,  0.2169,  0.0487, -0.1174,  0.1356,\n",
              "         -0.2296, -0.0540, -0.1482,  0.2189, -0.2510, -0.1314, -0.0608, -0.1677,\n",
              "         -0.0167, -0.1424,  0.3086, -0.2555, -0.2181, -0.2179, -0.3440,  0.2402],\n",
              "        [ 0.1703,  0.2136,  0.2920,  0.2369,  0.1323, -0.1768, -0.1284,  0.3658,\n",
              "          0.0279,  0.1940,  0.3567, -0.3645,  0.2829, -0.2980,  0.2451,  0.0178,\n",
              "         -0.2244,  0.2850, -0.0558, -0.3548, -0.1758,  0.2646, -0.2881,  0.3303,\n",
              "          0.2391,  0.0303, -0.1328, -0.2216,  0.1978, -0.3291, -0.3238, -0.2873],\n",
              "        [-0.3174,  0.2755, -0.0420, -0.2064, -0.2288, -0.0802, -0.0990, -0.3386,\n",
              "          0.0918,  0.2534,  0.3457,  0.1210, -0.1020,  0.0852, -0.2729, -0.1737,\n",
              "          0.2577, -0.1461,  0.3775, -0.0371,  0.0881, -0.0135, -0.0380,  0.2642,\n",
              "          0.2674, -0.3521, -0.3740, -0.0218,  0.2070,  0.3248,  0.0259,  0.3316],\n",
              "        [-0.3526, -0.0706,  0.1986, -0.1086, -0.3170,  0.1035,  0.0603,  0.3518,\n",
              "          0.1088, -0.1595,  0.3449,  0.0390,  0.3218,  0.1173,  0.0231, -0.1722,\n",
              "          0.0444,  0.0594,  0.1863, -0.2823,  0.0195,  0.2821, -0.3779, -0.0319,\n",
              "          0.2193,  0.0358,  0.3353,  0.1865, -0.0856, -0.3239, -0.1681, -0.3140],\n",
              "        [-0.2482, -0.1559,  0.2586,  0.1344,  0.2553, -0.1845, -0.2142,  0.1990,\n",
              "          0.1156, -0.1215, -0.2295, -0.0634, -0.3252, -0.3578, -0.0488, -0.0474,\n",
              "          0.2460,  0.0204,  0.0260,  0.1257,  0.3152, -0.0494, -0.2959,  0.0092,\n",
              "         -0.1907, -0.2380,  0.1957,  0.1569, -0.2734,  0.1229,  0.3030,  0.1665],\n",
              "        [-0.3717, -0.3445, -0.2575, -0.0794, -0.2970, -0.2408, -0.0635,  0.0356,\n",
              "          0.2561,  0.1589, -0.0621,  0.1212,  0.0603, -0.0078, -0.2233,  0.1263,\n",
              "         -0.1743,  0.1322, -0.0151,  0.0528,  0.2965,  0.0909, -0.1277,  0.2400,\n",
              "          0.1895,  0.2838, -0.0213, -0.1467,  0.3055,  0.3672, -0.3712,  0.1051],\n",
              "        [-0.2900,  0.3754,  0.3640,  0.2266, -0.1621, -0.3252, -0.2617,  0.0250,\n",
              "         -0.3009,  0.0570, -0.2451, -0.1279,  0.2238, -0.1473,  0.2815, -0.0089,\n",
              "          0.1519, -0.2628, -0.2701,  0.2274,  0.0704, -0.0947, -0.0611, -0.3377,\n",
              "          0.0354, -0.2621, -0.2001,  0.3287,  0.2929,  0.3415,  0.3167,  0.3532]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT256yOuGfcn"
      },
      "source": [
        "bn_model = torch.nn.Sequential(linear1, bn1, relu, linear2, bn2, relu, linear3).to(device)\n",
        "nn_model = torch.nn.Sequential(nn_linear1, relu, nn_linear2, relu, nn_linear3).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtvgsmDFGv1v"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Is-DsrQG0E0",
        "outputId": "f0710c63-5f88-4d42-b33a-c7759c67e122"
      },
      "source": [
        "train_total_batch = len(train_loader)\n",
        "test_total_batch = len(test_loader)\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  bn_model.train()\n",
        "\n",
        "  for X, Y in train_loader:\n",
        "    X = X.view(-1, 28 * 28).to(device)\n",
        "    Y = Y.to(device)\n",
        "    \n",
        "    bn_optimizer.zero_grad()\n",
        "    bn_prediction = bn_model(X)\n",
        "    bn_loss = criterion(bn_prediction, Y)\n",
        "    bn_loss.backward()\n",
        "    bn_optimizer.step()\n",
        "\n",
        "    nn_optimizer.zero_grad()\n",
        "    nn_prediction = nn_model(X)\n",
        "    nn_loss = criterion(nn_prediction, Y)\n",
        "    nn_loss.backward()\n",
        "    nn_optimizer.step()\n",
        "\n",
        "  #no_grad : no gradient descent\n",
        "  with torch.no_grad():\n",
        "    bn_model.eval()\n",
        "\n",
        "    bn_loss, nn_loss, bn_acc, nn_acc = 0,0,0,0\n",
        "    for i, (X, Y) in enumerate(train_loader):\n",
        "      X = X.view(-1, 28 * 28).to(device)\n",
        "      Y = Y.to(device)\n",
        "\n",
        "      bn_prediction = bn_model(X)\n",
        "      bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "      bn_loss += criterion(bn_prediction, Y)\n",
        "      bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "      nn_prediction = nn_model(X)\n",
        "      nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "      nn_loss += criterion(nn_prediction, Y)\n",
        "      nn_acc += nn_correct_prediction.float().mean()\n",
        "        \n",
        "    bn_loss, nn_loss, bn_acc, nn_acc = bn_loss/train_total_batch, nn_loss / train_total_batch, bn_acc/train_total_batch, nn_acc/train_total_batch\n",
        "      \n",
        "    print('[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.4f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.4f)' % ((epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        \n",
        "    for i, (X, Y) in enumerate(test_loader):\n",
        "      X = X.view(-1, 28 * 28).to(device)\n",
        "      Y = Y.to(device)\n",
        "\n",
        "      bn_prediction = bn_model(X)\n",
        "      bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "      bn_loss += criterion(bn_prediction, Y)\n",
        "      bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "      nn_prediction = nn_model(X)\n",
        "      nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "      nn_loss += criterion(nn_prediction, Y)\n",
        "      nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "    bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n",
        "\n",
        "    print('[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.4f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.4f)' % ((epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.13324(bn_acc:0.9591) vs No Batchnorm Loss(Acc): nn_loss:0.18852(nn_acc:0.9432)\n",
            "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.14925(bn_acc:0.9580) vs No Batchnorm Loss(Acc): nn_loss:0.20879(nn_acc:0.9424)\n",
            "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10818(bn_acc:0.9676) vs No Batchnorm Loss(Acc): nn_loss:0.18498(nn_acc:0.9459)\n",
            "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.13441(bn_acc:0.9606) vs No Batchnorm Loss(Acc): nn_loss:0.20552(nn_acc:0.9450)\n",
            "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08601(bn_acc:0.9746) vs No Batchnorm Loss(Acc): nn_loss:0.15368(nn_acc:0.9551)\n",
            "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.10789(bn_acc:0.9707) vs No Batchnorm Loss(Acc): nn_loss:0.19017(nn_acc:0.9502)\n",
            "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07971(bn_acc:0.9758) vs No Batchnorm Loss(Acc): nn_loss:0.16946(nn_acc:0.9513)\n",
            "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.11073(bn_acc:0.9689) vs No Batchnorm Loss(Acc): nn_loss:0.20013(nn_acc:0.9468)\n",
            "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07079(bn_acc:0.9776) vs No Batchnorm Loss(Acc): nn_loss:0.12976(nn_acc:0.9624)\n",
            "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.10326(bn_acc:0.9733) vs No Batchnorm Loss(Acc): nn_loss:0.16719(nn_acc:0.9598)\n",
            "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06284(bn_acc:0.9806) vs No Batchnorm Loss(Acc): nn_loss:0.13381(nn_acc:0.9615)\n",
            "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.09904(bn_acc:0.9724) vs No Batchnorm Loss(Acc): nn_loss:0.18347(nn_acc:0.9561)\n",
            "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05242(bn_acc:0.9836) vs No Batchnorm Loss(Acc): nn_loss:0.12207(nn_acc:0.9659)\n",
            "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.09648(bn_acc:0.9731) vs No Batchnorm Loss(Acc): nn_loss:0.17910(nn_acc:0.9590)\n",
            "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06230(bn_acc:0.9798) vs No Batchnorm Loss(Acc): nn_loss:0.14092(nn_acc:0.9616)\n",
            "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.10353(bn_acc:0.9689) vs No Batchnorm Loss(Acc): nn_loss:0.19035(nn_acc:0.9557)\n",
            "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05204(bn_acc:0.9835) vs No Batchnorm Loss(Acc): nn_loss:0.12031(nn_acc:0.9665)\n",
            "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.10028(bn_acc:0.9723) vs No Batchnorm Loss(Acc): nn_loss:0.19749(nn_acc:0.9553)\n",
            "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04657(bn_acc:0.9848) vs No Batchnorm Loss(Acc): nn_loss:0.12059(nn_acc:0.9652)\n",
            "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.09126(bn_acc:0.9743) vs No Batchnorm Loss(Acc): nn_loss:0.18522(nn_acc:0.9578)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bialt4PuewO5"
      },
      "source": [
        "dropout이 항상 성능의 향상은 x\n",
        "가중치 초기화는 의미있는듯!\n",
        "\n",
        "단순히 배치 정규화 진행했을때보다 높은 성능 도출\n",
        "\n",
        "하지만 합쳐졌을때 더 유의미한 결과는 아니었음\n",
        "-> 둘다 너무 높아서 그런가...?\n",
        "-> 아니면 초기화만 의미가 있었을지도 모른다.\n",
        "\n",
        "층을 더 늘려서 실험해본 후 dropout 적용해보기."
      ]
    }
  ]
}